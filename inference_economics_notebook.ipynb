{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "98LZ4qwYlkGA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from sympy import divisors\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from typing import Callable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language model and GPU definitions\n",
        "\n",
        "This section contains a transformer class and a GPU class, along with object definitions for important models and GPUs. Details such as whether a GPU should be assumed to have a kernel launch latency or what the latency structure of GPU collectives is like are also handled here.\n",
        "\n",
        "By default, we use the latency function from the NCCL Tests repository. These might correspond to collective kernels that have been insufficiently optimized - to get results in a more optimized setting, you can adjust the functions implementing time taken per collective as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An7GK3zR44gN",
        "outputId": "f962ccda-4ce5-437d-d99f-e0e845e3d063"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, name=\"None\", d_model=3*2**12, d_ff=9*2**12, ff_matrix_count=(1, 1), layers=120, n_experts=1, n_active_experts=1, num_query_heads=128, group_size=1, \\\n",
        "                 weight_precision_bytes=2, activation_precision_bytes=2, d_head=None, vocab_size=0, parallel_attention=False):\n",
        "        assert num_query_heads % group_size == 0\n",
        "\n",
        "        # Variables directly set\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.layers = layers\n",
        "        self.n_experts = n_experts\n",
        "        self.n_active_experts = n_active_experts\n",
        "        self.num_query_heads = num_query_heads\n",
        "        self.group_size = group_size\n",
        "        self.weight_precision_bytes = weight_precision_bytes\n",
        "        self.activation_precision_bytes = activation_precision_bytes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ff_matrix_count = ff_matrix_count\n",
        "        self.parallel_attention = parallel_attention\n",
        "\n",
        "        # Derived variables\n",
        "        self.ff_params_per_layer_per_expert = sum(self.ff_matrix_count) * self.d_model * self.d_ff\n",
        "        self.sparsity_factor = self.n_experts // self.n_active_experts\n",
        "        self.total_ff_params = self.layers * self.n_experts * self.ff_params_per_layer_per_expert\n",
        "        self.num_kv_heads = 2 * self.num_query_heads//self.group_size\n",
        "        self.d_head = d_head if d_head != None else self.d_model // self.num_query_heads\n",
        "        self.d_all_attn_heads = (self.num_query_heads + self.num_kv_heads) * self.d_head\n",
        "        self.attn_params_per_layer = self.d_all_attn_heads * self.d_model + self.d_head*self.num_query_heads*self.d_model\n",
        "\n",
        "        self.embedding_params = self.vocab_size * self.d_model * 2\n",
        "        self.total_attn_params = self.layers * self.attn_params_per_layer\n",
        "        self.total_params = self.total_attn_params + self.total_ff_params + self.embedding_params\n",
        "        self.total_active_params = self.total_attn_params + self.total_ff_params//self.sparsity_factor + self.embedding_params\n",
        "\n",
        "        self.kv_cache_size_per_input_bytes = self.num_kv_heads*self.d_head*self.layers*self.activation_precision_bytes\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "    def __repr__(self):\n",
        "        representation = f\"\"\"Model Details:\n",
        "        Name: {self.name}\n",
        "        d_model: {self.d_model}\n",
        "        d_ff: {self.d_ff}\n",
        "        Depth: {self.layers}\n",
        "        Total FF Params: {self.total_ff_params}\n",
        "        Total Embedding Params: {self.embedding_params}\n",
        "        Num Attention Heads: {self.num_query_heads}\n",
        "        d_head: {self.d_head}\n",
        "        Group size: {self.group_size}\n",
        "        Total Attention Params: {self.total_attn_params}\n",
        "        Total Params: {self.total_params}\n",
        "        Total Active Params: {self.total_active_params}\n",
        "        \"\"\"\n",
        "        return representation\n",
        "\n",
        "    def arithmetic_cost_flop(self, input_len, batch_size, seq_len=1):\n",
        "        mean_input_len = (input_len + (input_len + seq_len - 1))/2\n",
        "        return (2*self.total_active_params*batch_size*seq_len + 2*self.d_head*self.num_query_heads*self.layers*mean_input_len*batch_size*seq_len)\n",
        "\n",
        "    def memory_reads_writes_bytes(self, input_len, batch_size, tp_first_dim, tp_second_dim):\n",
        "        kv_cache_size_bytes = self.kv_cache_size_per_input_bytes*input_len*batch_size\n",
        "\n",
        "        used_experts_fraction = (1 - (1 - 1/self.sparsity_factor)**batch_size)\n",
        "        wp = self.weight_precision_bytes\n",
        "        ap = self.activation_precision_bytes\n",
        "\n",
        "        feedforward_matmul_rw_bytes = used_experts_fraction * self.n_experts * sum(self.ff_matrix_count) * \\\n",
        "                                         (wp*self.d_model*self.d_ff + tp_second_dim*ap*self.d_model*batch_size/self.sparsity_factor + \\\n",
        "                                          tp_first_dim*ap*self.d_ff*batch_size/self.sparsity_factor)\n",
        "\n",
        "        qkv_matmul_rw_bytes = wp*self.d_model*self.d_all_attn_heads + tp_second_dim*ap*(self.d_model*batch_size) + tp_first_dim*(self.d_all_attn_heads*batch_size)\n",
        "\n",
        "        proj_matmul_rw_bytes = wp*self.d_head*self.num_query_heads*self.d_model + \\\n",
        "                               ap*(tp_second_dim*self.d_model*batch_size + tp_first_dim*self.d_head*self.num_query_heads*batch_size)\n",
        "\n",
        "        unembedding_bytes = wp*self.vocab_size*self.d_model\n",
        "\n",
        "        return kv_cache_size_bytes + self.layers*(feedforward_matmul_rw_bytes + qkv_matmul_rw_bytes + proj_matmul_rw_bytes) + unembedding_bytes\n",
        "\n",
        "GPT_4 = Model(name=\"GPT-4\",\n",
        "              d_model=12288,\n",
        "              d_ff=3*12288,\n",
        "              layers=120,\n",
        "              n_experts=16,\n",
        "              n_active_experts=2,\n",
        "              num_query_heads=96,\n",
        "              group_size=96,\n",
        "              d_head=3*12288//(2*96),\n",
        "              activation_precision_bytes=2,\n",
        "              weight_precision_bytes=2,\n",
        "              vocab_size=100256,\n",
        ")\n",
        "\n",
        "GPT_3_5 = Model(name=\"GPT 3.5\",\n",
        "                d_model=2**9 * 3**2,\n",
        "                d_ff=4 * 2**9 * 3**2,\n",
        "                layers=32,\n",
        "                n_experts=4,\n",
        "                n_active_experts=2,\n",
        "                num_query_heads=32,\n",
        "                group_size=32,\n",
        "                activation_precision_bytes=2,\n",
        "                weight_precision_bytes=2\n",
        ")\n",
        "\n",
        "GPT_3 = Model(name=\"GPT-3\",\n",
        "              d_model=12288,\n",
        "              d_ff=4*12288,\n",
        "              layers=96,\n",
        "              n_experts=1,\n",
        "              n_active_experts=1,\n",
        "              num_query_heads=96,\n",
        "              d_head=128,\n",
        "              activation_precision_bytes=2,\n",
        "              weight_precision_bytes=2,\n",
        "              vocab_size=50257\n",
        ")\n",
        "\n",
        "PaLM_540B = Model(name=\"PaLM 540B\",\n",
        "                  d_model=18432,\n",
        "                  d_ff=73728,\n",
        "                  ff_matrix_count=(2, 1),\n",
        "                  layers=118,\n",
        "                  n_experts=1,\n",
        "                  n_active_experts=1,\n",
        "                  num_query_heads=48,\n",
        "                  d_head=256,\n",
        "                  group_size=48,\n",
        "                  activation_precision_bytes=2,\n",
        "                  weight_precision_bytes=2,\n",
        "                  vocab_size=256000,\n",
        "                  parallel_attention=True\n",
        ")\n",
        "\n",
        "PaLM_8B = Model(name=\"PaLM 8B\",\n",
        "                d_model=4096,\n",
        "                d_ff=4*4096,\n",
        "                ff_matrix_count=(2, 1),\n",
        "                layers=32,\n",
        "                num_query_heads=16,\n",
        "                group_size=16,\n",
        "                d_head=256,\n",
        "                activation_precision_bytes=2,\n",
        "                weight_precision_bytes=1,\n",
        "                vocab_size=256000,\n",
        "                parallel_attention=True\n",
        ")\n",
        "\n",
        "Falcon = Model(name=\"Falcon 180B\",\n",
        "               d_model=14848,\n",
        "               d_ff=4*14848,\n",
        "               layers=80,\n",
        "               d_head=64,\n",
        "               num_query_heads=232,\n",
        "               group_size=232,\n",
        "               activation_precision_bytes=2,\n",
        "               weight_precision_bytes=2,\n",
        "               vocab_size=65024\n",
        ")\n",
        "\n",
        "GPT_J_6B = Model(name=\"GPT-J 6B\",\n",
        "                 d_model=4096,\n",
        "                 d_ff=4*4096,\n",
        "                 layers=28,\n",
        "                 num_query_heads=16,\n",
        "                 d_head=256,\n",
        "                 group_size=1,\n",
        "                 activation_precision_bytes=2,\n",
        "                 weight_precision_bytes=2,\n",
        "                 vocab_size=50257\n",
        ")\n",
        "\n",
        "Mixtral_8x22B = Model(name=\"Mixtral 8x22B\",\n",
        "                      d_model=6144,\n",
        "                      d_ff=16384,\n",
        "                      ff_matrix_count=(2, 1),\n",
        "                      layers=56,\n",
        "                      n_experts=8,\n",
        "                      n_active_experts=2,\n",
        "                      num_query_heads=48,\n",
        "                      d_head=128,\n",
        "                      group_size=6,\n",
        "                      activation_precision_bytes=2,\n",
        "                      weight_precision_bytes=2,\n",
        "                      vocab_size=32000\n",
        ")\n",
        "\n",
        "Mixtral_8x7B = Model(name=\"Mixtral 8x7B\",\n",
        "                      d_model=4096,\n",
        "                      d_ff=14336,\n",
        "                      ff_matrix_count=(2, 1),\n",
        "                      layers=32,\n",
        "                      n_experts=8,\n",
        "                      n_active_experts=2,\n",
        "                      num_query_heads=32,\n",
        "                      group_size=4,\n",
        "                     activation_precision_bytes=2,\n",
        "                      weight_precision_bytes=2,\n",
        "                      vocab_size=32000\n",
        ")\n",
        "\n",
        "Llama_3_8B = Model(name=\"LLaMa 3 8B\",\n",
        "                    d_model=4096,\n",
        "                    d_ff=14336,\n",
        "                    ff_matrix_count=(2, 1),\n",
        "                    layers=32,\n",
        "                    n_experts=1,\n",
        "                    n_active_experts=1,\n",
        "                    num_query_heads=32,\n",
        "                    group_size=4,\n",
        "                    activation_precision_bytes=2,\n",
        "                    weight_precision_bytes=2,\n",
        "                    vocab_size=128256\n",
        ")\n",
        "\n",
        "Llama_3_8B_8_bit = Model(name=\"LLaMa 3 8B\",\n",
        "                        d_model=4096,\n",
        "                        d_ff=14336,\n",
        "                        ff_matrix_count=(2, 1),\n",
        "                        layers=32,\n",
        "                        n_experts=1,\n",
        "                        n_active_experts=1,\n",
        "                        num_query_heads=32,\n",
        "                        group_size=4,\n",
        "                        activation_precision_bytes=1,\n",
        "                        weight_precision_bytes=2,\n",
        "                        vocab_size=128256\n",
        ")\n",
        "\n",
        "Llama_3_70B = Model(name=\"LLaMa 3 70B\",\n",
        "                    d_model=8192,\n",
        "                    d_ff=28672,\n",
        "                    ff_matrix_count=(2, 1),\n",
        "                    layers=80,\n",
        "                    n_experts=1,\n",
        "                    n_active_experts=1,\n",
        "                    num_query_heads=64,\n",
        "                    group_size=8,\n",
        "                    activation_precision_bytes=2,\n",
        "                    weight_precision_bytes=2,\n",
        "                    vocab_size=128256\n",
        ")\n",
        "\n",
        "Llama_3_70B_8_bit = Model(name=\"LLaMa 3 70B 8-bit\",\n",
        "                    d_model=8192,\n",
        "                    d_ff=28672,\n",
        "                    ff_matrix_count=(2, 1),\n",
        "                    layers=80,\n",
        "                    n_experts=1,\n",
        "                    n_active_experts=1,\n",
        "                    num_query_heads=64,\n",
        "                    group_size=8,\n",
        "                    activation_precision_bytes=2,\n",
        "                    weight_precision_bytes=1,\n",
        "                    vocab_size=128256\n",
        ")\n",
        "\n",
        "Llama_3_70B_4_bit = Model(name=\"LLaMa 3 70B 4-bit\",\n",
        "                    d_model=8192,\n",
        "                    d_ff=28672,\n",
        "                    ff_matrix_count=(2, 1),\n",
        "                    layers=80,\n",
        "                    n_experts=1,\n",
        "                    n_active_experts=1,\n",
        "                    num_query_heads=64,\n",
        "                    group_size=8,\n",
        "                    activation_precision_bytes=2,\n",
        "                    weight_precision_bytes=0.5,\n",
        "                    vocab_size=128256\n",
        ")\n",
        "\n",
        "Llama_3_405B = Model(name=\"LLaMa 3 405B\",\n",
        "                    d_model=16384,\n",
        "                    d_ff=53248,\n",
        "                    ff_matrix_count=(2, 1),\n",
        "                    layers=126,\n",
        "                    n_experts=1,\n",
        "                    n_active_experts=1,\n",
        "                    num_query_heads=128,\n",
        "                    group_size=8,\n",
        "                    activation_precision_bytes=2,\n",
        "                    weight_precision_bytes=2,\n",
        "                    vocab_size=128256\n",
        ")\n",
        "\n",
        "Llama_3_405B_8_bit = Model(name=\"LLaMa 3 405B\",\n",
        "                    d_model=16384,\n",
        "                    d_ff=53248,\n",
        "                    ff_matrix_count=(2, 1),\n",
        "                    layers=126,\n",
        "                    n_experts=1,\n",
        "                    n_active_experts=1,\n",
        "                    num_query_heads=128,\n",
        "                    group_size=8,\n",
        "                    activation_precision_bytes=2,\n",
        "                    weight_precision_bytes=1,\n",
        "                    vocab_size=128256\n",
        ")\n",
        "\n",
        "def scale_model(model: Model, scale_factor: float):\n",
        "    d_model = model.d_model * scale_factor**(1/3)  \n",
        "    d_ff = model.d_ff * scale_factor**(1/3)\n",
        "    layers = int(model.layers * scale_factor**(1/3))\n",
        "\n",
        "    num_query_heads = int(model.num_query_heads * scale_factor**(1/3))\n",
        "    group_size = 1\n",
        "\n",
        "    return Model(name=model.name,\n",
        "                 d_model=d_model,\n",
        "                 d_ff=d_ff,\n",
        "                 ff_matrix_count=model.ff_matrix_count,\n",
        "                 layers=layers,\n",
        "                 n_experts=model.n_experts,\n",
        "                 n_active_experts=model.n_active_experts,\n",
        "                 num_query_heads=num_query_heads,\n",
        "                 group_size=group_size,\n",
        "                 d_head=model.d_head,\n",
        "                 weight_precision_bytes=model.weight_precision_bytes,\n",
        "                 activation_precision_bytes=model.activation_precision_bytes,\n",
        "                 vocab_size=model.vocab_size,\n",
        "                 parallel_attention=model.parallel_attention\n",
        "                 )\n",
        "\n",
        "class GPU:\n",
        "    name: str\n",
        "    flop_per_second: float\n",
        "    theoretical_flop_per_second: float\n",
        "    hbm_bandwidth_Bps: float\n",
        "    theoretical_hbm_bandwidth_Bps: float\n",
        "    hbm_size_bytes: float\n",
        "    l2_bandwidth_Bps: float\n",
        "    l2_cache_size_bytes: float\n",
        "    intranode_allreduce_bandwidth_Bps: float\n",
        "    internode_allreduce_bandwidth_Bps: float\n",
        "    node_size: int\n",
        "    price_dollars_per_hour: float\n",
        "    kernel_launch_latency_seconds: float\n",
        "    collective_time_seconds: Callable[[float, float, float, 'GPU', str, tuple[int]], float]\n",
        "\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 flop_per_second: dict,\n",
        "                 hbm_bandwidth_Bps: float,\n",
        "                 hbm_size_bytes: float,\n",
        "                 l2_bandwidth_Bps: float,\n",
        "                 l2_cache_size_bytes: float,\n",
        "                 intranode_allreduce_bandwidth_Bps: float,\n",
        "                 internode_allreduce_bandwidth_Bps: float,\n",
        "                 node_size: int,\n",
        "                 price_dollars_per_hour: float,\n",
        "                 kernel_launch_latency_seconds: float,\n",
        "                 collective_time_seconds: Callable[[float, float, float, 'GPU', str, tuple[int]], float],\n",
        "                 arithmetic_utilization_cap=1,\n",
        "                 memory_bwd_utilization_cap=1):\n",
        "        self.name = name\n",
        "        self.theoretical_flop_per_second = flop_per_second\n",
        "        self.theoretical_hbm_bandwidth_Bps = hbm_bandwidth_Bps\n",
        "        self.flop_per_second = {p: flop_per_second[p]*arithmetic_utilization_cap for p in flop_per_second}\n",
        "        self.hbm_bandwidth_Bps = hbm_bandwidth_Bps*memory_bwd_utilization_cap\n",
        "        self.hbm_size_bytes = hbm_size_bytes\n",
        "        self.l2_bandwidth_Bps = l2_bandwidth_Bps\n",
        "        self.l2_cache_size_bytes = l2_cache_size_bytes\n",
        "        self.intranode_allreduce_bandwidth_Bps = intranode_allreduce_bandwidth_Bps\n",
        "        self.internode_allreduce_bandwidth_Bps = internode_allreduce_bandwidth_Bps\n",
        "        self.node_size = node_size\n",
        "        self.price_dollars_per_hour = price_dollars_per_hour\n",
        "        self.kernel_launch_latency_seconds = kernel_launch_latency_seconds\n",
        "        self.collective_time_seconds = collective_time_seconds\n",
        "        self.arithmetic_utilization_cap = arithmetic_utilization_cap\n",
        "        self.memory_bwd_utilization_cap = memory_bwd_utilization_cap\n",
        "\n",
        "def collective_latency_nccl_seconds(nRanks, nNodes, coll=\"allreduce\", algo=\"LL\", ll_base_latency_seconds=6.8e-6, ll128_base_latency_seconds=14e-6):\n",
        "   if coll == \"allreduce\":\n",
        "     m = 2\n",
        "   else:\n",
        "     m = 1\n",
        "\n",
        "   if algo == \"LL\":\n",
        "      return m * ((nRanks/nNodes - 1)*0.6e-6 + 5e-6*np.log2(nNodes)) + 6.8e-6\n",
        "   elif algo == \"LL128\":\n",
        "      return m * ((nRanks/nNodes - 1)*1.25e-6 + 8.5e-6*np.log2(nNodes)) + 14e-6\n",
        "   elif algo == \"Simple\":\n",
        "      return m * ((nRanks/nNodes - 1)*28e-6 + 28e-6*np.log2(nNodes))\n",
        "   else:\n",
        "      raise ValueError(\"Algorithm code given to collective_latency_nccl_seconds is invalid.\")\n",
        "\n",
        "def mean_collective_time_nccl_seconds(nRanks, nNodes, bytes_reduced, gpu: GPU, coll=\"allreduce\", latency_bw_weights=(1, 1), ll_base_latency_seconds=6.8e-6, ll128_base_latency_seconds=14e-6):\n",
        "    result = np.infty * np.ones(shape=np.shape(nRanks))\n",
        "    weighted_result = np.infty * np.ones(shape=np.shape(nRanks))\n",
        "\n",
        "    assert np.all(nNodes <= np.maximum(1, nRanks)) and np.all(nNodes >= nRanks/gpu.node_size)\n",
        "\n",
        "    if coll == \"allreduce\":\n",
        "        m = 2\n",
        "    else:\n",
        "        m = 1\n",
        "\n",
        "    algorithm_bandwidth_factors = {\"Simple\": 1,\n",
        "                                    \"LL128\": 0.95,\n",
        "                                    \"LL\": 0.5}\n",
        "\n",
        "    if coll == \"p2p\":\n",
        "       assert np.all(nRanks == 2)\n",
        "\n",
        "    lat_weight, bw_weight = latency_bw_weights\n",
        "\n",
        "    for algo in algorithm_bandwidth_factors:\n",
        "      bw_factor = algorithm_bandwidth_factors[algo]\n",
        "      curr_latency_time_seconds = collective_latency_nccl_seconds(nRanks, nNodes, coll=coll, algo=algo, ll_base_latency_seconds=ll_base_latency_seconds, ll128_base_latency_seconds=ll128_base_latency_seconds)\n",
        "      curr_bw_time_seconds = nNodes*np.maximum(0, nRanks/nNodes - 1)*bytes_reduced/(nRanks*gpu.intranode_allreduce_bandwidth_Bps*bw_factor) + \\\n",
        "                             (nNodes-1)*bytes_reduced/(nRanks*gpu.internode_allreduce_bandwidth_Bps*bw_factor)\n",
        "      \n",
        "      if coll != \"allreduce\":\n",
        "       curr_bw_time_seconds /= 2\n",
        "\n",
        "      nonlinear_correction_time_seconds = np.zeros(shape=np.shape(nRanks))\n",
        "\n",
        "      curr_result = curr_latency_time_seconds + curr_bw_time_seconds + nonlinear_correction_time_seconds\n",
        "      smaller_indices = np.where(curr_result < result)\n",
        "\n",
        "      result = np.minimum(result, curr_latency_time_seconds + curr_bw_time_seconds + nonlinear_correction_time_seconds)\n",
        "\n",
        "      weighted_result[smaller_indices] = lat_weight*curr_latency_time_seconds[smaller_indices] + \\\n",
        "                                         bw_weight*curr_bw_time_seconds[smaller_indices] + \\\n",
        "                                         bw_weight*nonlinear_correction_time_seconds[smaller_indices]\n",
        "\n",
        "    result[np.where(nRanks <= 1)] = 0\n",
        "    weighted_result[np.where(nRanks <= 1)] = 0\n",
        "\n",
        "    return weighted_result\n",
        "\n",
        "def tpu_collective_time_seconds(nRanks, nNodes, bytes_reduced, gpu: GPU, coll=\"allreduce\", latency_bw_weights=(1, 1)):\n",
        "    if coll == \"allreduce\":\n",
        "        m = 2\n",
        "    else:\n",
        "        m = 1\n",
        "\n",
        "    if coll == \"p2p\":\n",
        "        assert np.all(nRanks == 2)\n",
        "\n",
        "    lat_weight, bw_weight = latency_bw_weights\n",
        "\n",
        "    curr_latency_time_seconds = m*(nRanks-1)*1e-6\n",
        "    curr_bw_time_seconds = (nRanks-1)*bytes_reduced/(nRanks*gpu.internode_allreduce_bandwidth_Bps)\n",
        "\n",
        "    if coll != \"allreduce\":\n",
        "       curr_bw_time_seconds /= 2\n",
        "\n",
        "    nonlinear_correction_time_seconds = np.zeros(shape=np.shape(nRanks)) # 1e-6 * bytes_reduced**0.3345\n",
        "    result = lat_weight*curr_latency_time_seconds + bw_weight*curr_bw_time_seconds + bw_weight*nonlinear_correction_time_seconds\n",
        "\n",
        "    result[np.where(nRanks <= 1)] = 0\n",
        "    return result\n",
        "\n",
        "\n",
        "H100 = GPU(name=\"H100\",\n",
        "           flop_per_second={4: 2e15, 8: 2e15, 16: 1e15},\n",
        "           hbm_bandwidth_Bps=3.3e12,\n",
        "           hbm_size_bytes=8e10,\n",
        "           l2_cache_size_bytes=2.5e7,\n",
        "           l2_bandwidth_Bps=1.2e13,\n",
        "           intranode_allreduce_bandwidth_Bps=9e11/4,\n",
        "           internode_allreduce_bandwidth_Bps=5e10/2,\n",
        "           node_size=8,\n",
        "           price_dollars_per_hour=3.15*2/3,\n",
        "           kernel_launch_latency_seconds=4e-6,\n",
        "           collective_time_seconds=mean_collective_time_nccl_seconds,\n",
        "           arithmetic_utilization_cap=1,\n",
        "           memory_bwd_utilization_cap=1\n",
        ")\n",
        "\n",
        "H100_ZL = GPU(name=\"H100 ZL\",\n",
        "           flop_per_second={4: 2e15, 8: 2e15, 16: 1e15},\n",
        "           hbm_bandwidth_Bps=3.3e12,\n",
        "           hbm_size_bytes=8e10,\n",
        "           l2_cache_size_bytes=2.5e7,\n",
        "           l2_bandwidth_Bps=1.2e13,\n",
        "           intranode_allreduce_bandwidth_Bps=9e11/4,\n",
        "           internode_allreduce_bandwidth_Bps=5e10/2,\n",
        "           node_size=8,\n",
        "           price_dollars_per_hour=3.15*2/3,\n",
        "           kernel_launch_latency_seconds=0,\n",
        "           collective_time_seconds=lambda x, y, z, gpu, coll, latency_bw_weights: mean_collective_time_nccl_seconds(x, y, z, gpu, coll, latency_bw_weights, ll_base_latency_seconds=0, ll128_base_latency_seconds=0),\n",
        "           arithmetic_utilization_cap=1,\n",
        "           memory_bwd_utilization_cap=1\n",
        ")\n",
        "\n",
        "H200 = GPU(name=\"H200\",\n",
        "           flop_per_second={4: 2e15, 8: 2e15, 16: 1e15},\n",
        "           hbm_bandwidth_Bps=4.8e12,\n",
        "           hbm_size_bytes=141e9,\n",
        "           l2_cache_size_bytes=2.5e7,\n",
        "           l2_bandwidth_Bps=1.2e13,\n",
        "           intranode_allreduce_bandwidth_Bps=9e11/4,\n",
        "           internode_allreduce_bandwidth_Bps=5e10/2,\n",
        "           node_size=8,\n",
        "           price_dollars_per_hour=3.15*2/3,\n",
        "           kernel_launch_latency_seconds=4e-6,\n",
        "           collective_time_seconds=mean_collective_time_nccl_seconds,\n",
        "           arithmetic_utilization_cap=1,\n",
        "           memory_bwd_utilization_cap=1)\n",
        "      \n",
        "\n",
        "A100 = GPU(name=\"A100\",\n",
        "           flop_per_second={8: 6.24e14, 16: 3.12e14},\n",
        "           hbm_bandwidth_Bps=2e12,\n",
        "           hbm_size_bytes=8e10,\n",
        "           l2_cache_size_bytes=2.5e7,\n",
        "           l2_bandwidth_Bps=1.2e13,\n",
        "           intranode_allreduce_bandwidth_Bps=6e11/4,\n",
        "           internode_allreduce_bandwidth_Bps=2.5e10/2,\n",
        "           node_size=8,\n",
        "           price_dollars_per_hour=2.26*2/3,\n",
        "           kernel_launch_latency_seconds=4e-6,\n",
        "           collective_time_seconds=mean_collective_time_nccl_seconds,\n",
        "           arithmetic_utilization_cap=1,\n",
        "           memory_bwd_utilization_cap=1\n",
        ")\n",
        "V100 = GPU(name=\"V100\",\n",
        "           flop_per_second={8: 1e14, 16: 1e14},\n",
        "           hbm_bandwidth_Bps=9e11,\n",
        "           hbm_size_bytes=1.6e10,\n",
        "           l2_cache_size_bytes=2.5e7,\n",
        "           l2_bandwidth_Bps=3e12,\n",
        "           intranode_allreduce_bandwidth_Bps=3e11/4,\n",
        "           internode_allreduce_bandwidth_Bps=1.25e10/2,\n",
        "           node_size=8,\n",
        "           price_dollars_per_hour=0.63*2/3,\n",
        "           kernel_launch_latency_seconds=4e-6,\n",
        "           collective_time_seconds=mean_collective_time_nccl_seconds,\n",
        "           arithmetic_utilization_cap=1,\n",
        "           memory_bwd_utilization_cap=1\n",
        ")\n",
        "TPU_v4 = GPU(name=\"TPU v4\",\n",
        "             flop_per_second={8: 2.6e14, 16: 2.6e14},\n",
        "             hbm_bandwidth_Bps=1.6e12,\n",
        "             hbm_size_bytes=32e9,\n",
        "             l2_cache_size_bytes=5e4,\n",
        "             l2_bandwidth_Bps=6.44e12,\n",
        "             intranode_allreduce_bandwidth_Bps=np.infty,\n",
        "             internode_allreduce_bandwidth_Bps=2.5e11,\n",
        "             node_size=1,\n",
        "             price_dollars_per_hour=1,\n",
        "             kernel_launch_latency_seconds=4e-6,\n",
        "             collective_time_seconds=tpu_collective_time_seconds,\n",
        ")\n",
        "Groq_LPU = GPU(name=\"Groq LPU\",\n",
        "               flop_per_second={8: 750e12, 16: 188e12},\n",
        "               hbm_bandwidth_Bps=8e13,\n",
        "               hbm_size_bytes=230e6,\n",
        "               l2_cache_size_bytes=0,\n",
        "               l2_bandwidth_Bps=np.infty,\n",
        "               intranode_allreduce_bandwidth_Bps=np.infty,\n",
        "               internode_allreduce_bandwidth_Bps=330e9/2,\n",
        "               node_size=1,\n",
        "               price_dollars_per_hour=1,\n",
        "               kernel_launch_latency_seconds=4e-6,\n",
        "               collective_time_seconds=tpu_collective_time_seconds,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG_7rZCtmD43"
      },
      "source": [
        "## Token latency functions\n",
        "\n",
        "This section implements all of the token latency functions that are discussed in the paper, and an additional more complex one that handles complications such as pipeline parallelism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "faIHWZKll-zM"
      },
      "outputs": [],
      "source": [
        "def section_2_2_token_latency_seconds(N_GPU, model: Model, gpu: GPU, batch_size, input_len=0):\n",
        "   t_reduce_seconds = 2e-6 * np.maximum(N_GPU**(1/2) - 1, 0) + 6e-6\n",
        "   t_matmul_seconds = 0\n",
        "\n",
        "   if model.parallel_attention:\n",
        "      allreduces_per_layer = 2\n",
        "   else:\n",
        "      allreduces_per_layer = 4\n",
        "\n",
        "   result = model.layers*allreduces_per_layer*t_reduce_seconds + np.maximum(model.weight_precision_bytes*model.total_params/(N_GPU*gpu.hbm_bandwidth_Bps), \\\n",
        "                                                                                    2*model.total_params*batch_size/(N_GPU*gpu.flop_per_second[8*model.weight_precision_bytes]))\n",
        "   kv_cache_size_bytes = model.kv_cache_size_per_input_bytes * input_len * batch_size\n",
        "   result[np.where(N_GPU * gpu.hbm_size_bytes < kv_cache_size_bytes + model.weight_precision_bytes*model.total_params)] = float(\"inf\")\n",
        "\n",
        "   return result\n",
        "\n",
        "\"\"\"\n",
        "The token latency function as presented in the paper.\n",
        "This implementation follows the description in Section 3.5 exactly.\n",
        "\"\"\"\n",
        "def token_latency_seconds_as_presented_in_paper(N_GPU, model: Model, gpu: GPU, batch_size, input_len=0, seq_len=1):\n",
        "   serial_matmuls_per_layer = 4\n",
        "   nRanks = N_GPU**(1/2)\n",
        "   nNodes = np.ceil(N_GPU/gpu.node_size)**(1/2)\n",
        "   nParallelReduces = N_GPU/nRanks\n",
        "\n",
        "   words_reduced = [model.d_head*(model.num_kv_heads + model.num_query_heads)/nParallelReduces, model.d_model/nParallelReduces, \\\n",
        "                    model.n_active_experts*model.ff_matrix_count[0]*model.d_ff/nParallelReduces, \\\n",
        "                    model.n_active_experts*model.ff_matrix_count[1]*model.d_model/nParallelReduces]\n",
        "\n",
        "   arithmetic_cost_flop = model.arithmetic_cost_flop(input_len, batch_size*seq_len)\n",
        "   kv_cache_size_bytes = model.kv_cache_size_per_input_bytes * input_len * batch_size\n",
        "\n",
        "   memory_rw_bytes = model.memory_reads_writes_bytes(input_len, batch_size*seq_len, N_GPU**(1/2), N_GPU**(1/2))\n",
        "\n",
        "   network_comm_time_sec = np.zeros(shape=np.shape(N_GPU))\n",
        "   network_bw_time_sec = np.zeros(shape=np.shape(N_GPU))\n",
        "   network_latency_time_sec = np.zeros(shape=np.shape(N_GPU))\n",
        "\n",
        "   for words_reduced_sample in words_reduced:\n",
        "       bytes_reduced = words_reduced_sample * batch_size * seq_len * model.activation_precision_bytes\n",
        "       network_comm_time_sec += model.layers * gpu.collective_time_seconds(nRanks, nNodes, bytes_reduced, gpu, \"allreduce\", latency_bw_weights=(1, 1))\n",
        "\n",
        "       network_bw_time_sec += model.layers * gpu.collective_time_seconds(nRanks, nNodes, bytes_reduced, gpu, \"allreduce\", latency_bw_weights=(0, 1))\n",
        "       network_latency_time_sec += model.layers * gpu.collective_time_seconds(nRanks, nNodes, bytes_reduced, gpu, \"allreduce\", latency_bw_weights=(1, 0))\n",
        "\n",
        "   result = (model.layers*serial_matmuls_per_layer*gpu.kernel_launch_latency_seconds) + network_comm_time_sec \\\n",
        "            + np.maximum( memory_rw_bytes/(N_GPU*gpu.hbm_bandwidth_Bps), \\\n",
        "                              arithmetic_cost_flop/(N_GPU*gpu.flop_per_second[8*model.weight_precision_bytes]))\n",
        "\n",
        "   kv_cache_size_bytes = model.kv_cache_size_per_input_bytes * input_len * batch_size\n",
        "   result[np.where(N_GPU * gpu.hbm_size_bytes < kv_cache_size_bytes + model.weight_precision_bytes*model.total_params)] = float(\"inf\")\n",
        "\n",
        "   return result\n",
        "\n",
        "\"\"\"\n",
        "This token latency function has some additional capabilities over the one presented in the paper.\n",
        "It enables turning 2D TP on/off and properly handles pipeline parallelism.\n",
        "These make a small difference when working with NVIDIA hardware but are important if we want to apply the model to e.g. Groq LPUs.\n",
        "\"\"\"\n",
        "def final_token_latency_seconds(N_GPU, model: Model, gpu: GPU, batch_size, input_len=0, seq_len=1):\n",
        "   result = float(\"inf\") * np.ones(shape=np.shape(N_GPU))\n",
        "   if np.shape(batch_size) != np.shape(N_GPU):\n",
        "      batch_size = batch_size * np.ones(shape=np.shape(N_GPU))\n",
        "\n",
        "   for two_d_tensor_parallel in [True, False]:\n",
        "    for N_PP in [1, 2, 4]:\n",
        "        N_TP = N_GPU/N_PP\n",
        "        interleaving_factor = 1 # redundant modeling of pipeline interleaving\n",
        "\n",
        "        for num_of_microbatches in N_PP * np.logspace(0, 4, num=5, base=2):\n",
        "          microbatch_size = batch_size/num_of_microbatches\n",
        "          \n",
        "          bubble_fraction = (N_PP - 1)/(N_PP - 1 + interleaving_factor*num_of_microbatches)\n",
        "          serial_matmuls_per_layer = 4\n",
        "\n",
        "          if two_d_tensor_parallel:\n",
        "              nRanks = N_TP**(1/2)\n",
        "              nParallelReduces = N_TP/nRanks\n",
        "              nNodes = np.ceil(N_TP/gpu.node_size)**(1/2)\n",
        "\n",
        "              words_reduced = [model.d_head*(model.num_kv_heads + model.num_query_heads)/nParallelReduces, model.d_model/nParallelReduces, \\\n",
        "                               model.n_active_experts*model.ff_matrix_count[0]*model.d_ff/nParallelReduces, \\\n",
        "                               model.n_active_experts*model.ff_matrix_count[1]*model.d_model/nParallelReduces]\n",
        "          else:\n",
        "              nRanks = N_TP\n",
        "              nParallelReduces = N_TP/nRanks\n",
        "              nNodes = np.ceil(N_TP/gpu.node_size)\n",
        "\n",
        "              words_reduced = [model.d_model/nParallelReduces, model.n_active_experts*model.ff_matrix_count[1]*model.d_model/nParallelReduces]\n",
        "\n",
        "          arithmetic_cost_flop = model.arithmetic_cost_flop(input_len, batch_size*seq_len)\n",
        "          kv_cache_size_bytes = model.kv_cache_size_per_input_bytes * input_len * batch_size\n",
        "\n",
        "          if two_d_tensor_parallel:\n",
        "            memory_rw_bytes = num_of_microbatches * model.memory_reads_writes_bytes(input_len, microbatch_size*seq_len, N_TP**(1/2), N_TP**(1/2))\n",
        "          else:\n",
        "            memory_rw_bytes = num_of_microbatches * model.memory_reads_writes_bytes(input_len, microbatch_size*seq_len, N_TP, 1)\n",
        "\n",
        "          network_comm_time_sec = np.zeros(shape=np.shape(N_GPU))\n",
        "\n",
        "          for words_reduced_sample in words_reduced:\n",
        "             bytes_reduced = words_reduced_sample * microbatch_size * seq_len * model.activation_precision_bytes\n",
        "\n",
        "             network_comm_time_sec += model.layers * gpu.collective_time_seconds(nRanks, nNodes, bytes_reduced, gpu, \"allreduce\", latency_bw_weights=(1, 0))\n",
        "             network_comm_time_sec += (model.layers/N_PP) * num_of_microbatches * gpu.collective_time_seconds(nRanks, nNodes, bytes_reduced, gpu, \"allreduce\", latency_bw_weights=(0, 1))\n",
        "\n",
        "          pp_words_read = model.d_model * microbatch_size * seq_len / N_TP\n",
        "          pp_bytes_read = pp_words_read*model.activation_precision_bytes\n",
        "\n",
        "          network_comm_time_sec += (N_PP*interleaving_factor - 1) * gpu.collective_time_seconds(2*np.ones(shape=np.shape(N_GPU)), \\\n",
        "                                                                                                 2*np.ones(shape=np.shape(N_GPU)), pp_bytes_read, gpu, \"p2p\", latency_bw_weights=(1, 0))\n",
        "          network_comm_time_sec += (N_PP*interleaving_factor - 1) * (num_of_microbatches/N_PP) * gpu.collective_time_seconds(2*np.ones(shape=np.shape(N_GPU)), \\\n",
        "                                                                                                2*np.ones(shape=np.shape(N_GPU)), pp_bytes_read, gpu, \"p2p\", latency_bw_weights=(0, 1))\n",
        "          curr_result = (model.layers*serial_matmuls_per_layer*gpu.kernel_launch_latency_seconds) + 1/(1-bubble_fraction) * (network_comm_time_sec \\\n",
        "                        + np.maximum( memory_rw_bytes/(N_GPU*gpu.hbm_bandwidth_Bps), \\\n",
        "                                      arithmetic_cost_flop/(N_GPU*gpu.flop_per_second[8*model.weight_precision_bytes]))\n",
        "          )\n",
        "\n",
        "          kv_cache_size_bytes = model.kv_cache_size_per_input_bytes * input_len * batch_size\n",
        "          curr_result[np.where(N_GPU * gpu.hbm_size_bytes < kv_cache_size_bytes + model.weight_precision_bytes*model.total_params)] = float(\"inf\")\n",
        "          curr_result[np.where(N_PP > np.minimum(N_GPU, batch_size))] = float(\"inf\")\n",
        "\n",
        "          result = np.minimum(result, curr_result)\n",
        "   return result\n",
        "\n",
        "#token_latency_seconds_default = token_latency_seconds_as_presented_in_paper\n",
        "token_latency_seconds_default = final_token_latency_seconds\n",
        "\n",
        "def spec_dec_token_latency_seconds(N_GPU, model: Model, approx_model: Model, gpu: GPU, batch_size, acceptance_prob, gamma_max, input_len=0):\n",
        "   final_latency = token_latency_seconds_default(N_GPU, model, gpu, batch_size, input_len)\n",
        "   approx_token_latency = token_latency_seconds_default(N_GPU, approx_model, gpu, batch_size, input_len)\n",
        "\n",
        "   for gamma in range(1, gamma_max+1):\n",
        "     base_token_latency = token_latency_seconds_default(N_GPU, model, gpu, batch_size, input_len, seq_len=gamma+1)\n",
        "\n",
        "     total_latency = base_token_latency + approx_token_latency*gamma\n",
        "     expected_tokens_generated = (1 - acceptance_prob**(gamma+1))/(1 - acceptance_prob)\n",
        "     final_latency = np.minimum(final_latency, total_latency/expected_tokens_generated)\n",
        "\n",
        "   return final_latency\n",
        "\n",
        "def spec_dec_mfu_target_opt(target_mfu, model: Model, approx_model: Model, gpu: GPU, acceptance_prob, gamma_max, input_len=0):\n",
        "   def aux(x):\n",
        "      N_GPU, batch_size = np.exp(x)\n",
        "      N_GPU_a = np.array([N_GPU], dtype=float)\n",
        "      batch_size_a = np.array([batch_size], dtype=float)\n",
        "\n",
        "      latency_seconds = spec_dec_token_latency_seconds(N_GPU_a, model, approx_model, gpu, batch_size_a, acceptance_prob, gamma_max, input_len)[0]\n",
        "      flop_per_second = gpu.theoretical_flop_per_second[model.weight_precision_bytes*8]\n",
        "\n",
        "      if latency_seconds == float(\"inf\"):\n",
        "         return float(\"inf\")\n",
        "      else:\n",
        "         return np.log(latency_seconds) + 1e2 * (np.log(model.arithmetic_cost_flop(input_len, batch_size)/(latency_seconds*flop_per_second*N_GPU)) - np.log(target_mfu))**2\n",
        "\n",
        "   res = minimize(aux, x0=[4, 4], method=\"Powell\")\n",
        "\n",
        "   N_GPU, batch_size = np.exp(res.x)\n",
        "   N_GPU_a = np.array([N_GPU], dtype=float)\n",
        "   batch_size_a = np.array([batch_size], dtype=float)\n",
        "\n",
        "   latency_seconds = spec_dec_token_latency_seconds(N_GPU_a, model, approx_model, gpu, batch_size_a, acceptance_prob, gamma_max, input_len)[0]\n",
        "\n",
        "   return N_GPU, batch_size, latency_seconds\n",
        "\n",
        "def search_for_model(base_model: Model, approx_model: Model, gpu: GPU, acceptance_prob: float, gamma_max: int, input_len: int, target_mfu: float, target_tok_per_sec: float):\n",
        "   low = -4\n",
        "   high = 4\n",
        "\n",
        "   while high - low > 1e-2:\n",
        "      mid = low + (high - low)/2\n",
        "      current_model = scale_model(base_model, 10**mid)\n",
        "      N_GPU, batch_size, latency_seconds = spec_dec_mfu_target_opt(target_mfu, current_model, approx_model, gpu, acceptance_prob, gamma_max, input_len)\n",
        "      tok_per_sec = 1/latency_seconds\n",
        "\n",
        "      print(\"%.2e\" % (current_model.total_params), tok_per_sec, \"GPUs: %.2f\" % (N_GPU), \"batch size: %.2f\" % (batch_size))\n",
        "\n",
        "      if tok_per_sec < target_tok_per_sec:\n",
        "         high = mid\n",
        "      else:\n",
        "         low = mid\n",
        "\n",
        "   return current_model, N_GPU, batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.99e+10 124.99729192947106 GPUs: 10.98 batch size: 54.86\n",
            "7.79e+12 8.314054838307927 GPUs: 198.70 batch size: 151.58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/egeerdil/.pyenv/versions/3.10.13/lib/python3.10/site-packages/scipy/optimize/_optimize.py:2577: RuntimeWarning: invalid value encountered in scalar multiply\n",
            "  tmp2 = (x - v) * (fx - fw)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7.80e+11 45.55190466628142 GPUs: 24.00 batch size: 33.69\n",
            "2.48e+11 72.31317028761896 GPUs: 19.40 batch size: 54.08\n",
            "1.39e+11 108.36609920704419 GPUs: 10.72 batch size: 35.42\n",
            "1.86e+11 80.10522896574324 GPUs: 16.27 batch size: 54.51\n",
            "1.61e+11 103.04044928222497 GPUs: 10.54 batch size: 31.73\n",
            "1.51e+11 105.49720182801478 GPUs: 11.04 batch size: 34.66\n",
            "1.46e+11 106.51883748086776 GPUs: 10.61 batch size: 34.15\n",
            "1.43e+11 107.4735116937014 GPUs: 10.76 batch size: 34.98\n",
            "1.43e+11\n"
          ]
        }
      ],
      "source": [
        "approx_model = Llama_3_8B\n",
        "gpu = H100_ZL\n",
        "test_model, n_gpu, batch_size = search_for_model(Llama_3_70B, approx_model, gpu, acceptance_prob=0.8, gamma_max=4, input_len=0, target_mfu=0.1, target_tok_per_sec=107)\n",
        "print(\"%.2e\" % (test_model.total_params))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens per second: [107.47351169]\n",
            "Requests per second: [29.37187539]\n",
            "Throughput (overall tok/s): [3759.60005015]\n",
            "Throughput per GPU (overall tok/s): [349.44727005]\n",
            "Total latency: [1.19099114]\n",
            "Utilization rate: [0.09978408]\n",
            "Price per M output tokens (USD): [1.66930288]\n",
            "Output token latency (seconds): [0.00930462]\n"
          ]
        }
      ],
      "source": [
        "model_obj = test_model\n",
        "approx_model_obj = approx_model\n",
        "output_len = 128\n",
        "input_len = 0\n",
        "\n",
        "#ttft_seconds = model_obj.arithmetic_cost_flop(0, input_len*batch_size)/(n_gpu*gpu.flop_per_second[8*model_obj.weight_precision_bytes])\n",
        "ttft_seconds = 0 #token_latency_seconds_default(np.array([n_gpu]), model_obj, gpu, np.array([batch_size]), \\\n",
        "                 #                             input_len=0, seq_len=1)\n",
        "output_tok_latency_seconds = spec_dec_token_latency_seconds(np.array([n_gpu]), model_obj, approx_model_obj, gpu, np.array([batch_size]), \\\n",
        "                                                            acceptance_prob=0.8, gamma_max=4, input_len=input_len)\n",
        "\n",
        "total_latency_seconds = output_len*output_tok_latency_seconds\n",
        "\n",
        "print(\"Tokens per second:\", output_len/total_latency_seconds)\n",
        "print(\"Requests per second:\", batch_size/total_latency_seconds)\n",
        "print(\"Throughput (overall tok/s):\", output_len*batch_size/total_latency_seconds)\n",
        "print(\"Throughput per GPU (overall tok/s):\", output_len*batch_size/(total_latency_seconds*n_gpu))\n",
        "print(\"Total latency:\", total_latency_seconds)\n",
        "print(\"Utilization rate:\", (model_obj.arithmetic_cost_flop(input_len, output_len*batch_size))\n",
        "                            /(n_gpu*gpu.theoretical_flop_per_second[8*model_obj.weight_precision_bytes]*total_latency_seconds))\n",
        "print(\"Price per M output tokens (USD):\", 1e6 * gpu.price_dollars_per_hour / (3600*output_len*batch_size/(total_latency_seconds*n_gpu)))\n",
        "print(\"Output token latency (seconds):\", output_tok_latency_seconds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Details:\n",
            "        Name: Mixtral 8x22B\n",
            "        d_model: 6562.918941960688\n",
            "        d_ff: 17501.117178561835\n",
            "        Depth: 59\n",
            "        Total FF Params: 162639513426.30643\n",
            "        Total Embedding Params: 420026812.2854841\n",
            "        Num Attention Heads: 51\n",
            "        d_head: 128\n",
            "        Group size: 1\n",
            "        Total Attention Params: 10110885425.336172\n",
            "        Total Params: 173170425663.9281\n",
            "        Total Active Params: 51190790593.62166\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "print(test_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of GPUs: 47.99999654692841\n",
            "Batch size: 414.17651587908176\n",
            "Tokens per second: [9.82211032]\n",
            "Requests per second: [31.78193307]\n",
            "Throughput (overall tok/s): [4068.08743269]\n",
            "Throughput per GPU (overall tok/s): [84.75182761]\n",
            "Total latency: [13.03182267]\n",
            "Utilization rate: [0.14930502]\n",
            "Price per M output tokens (USD): [4.93816511]\n",
            "Output token latency (seconds): [0.10171001]\n"
          ]
        }
      ],
      "source": [
        "model_obj = GPT_4\n",
        "approx_model_obj = Llama_3_8B\n",
        "gpu = A100\n",
        "\n",
        "n_gpu, batch_size, latency_seconds = spec_dec_mfu_target_opt(0.15, model_obj, approx_model_obj, gpu, acceptance_prob=0.8, gamma_max=0, input_len=0)\n",
        "\n",
        "input_len = 0\n",
        "output_len = 128\n",
        "\n",
        "#ttft_seconds = model_obj.arithmetic_cost_flop(0, input_len*batch_size)/(n_gpu*gpu.flop_per_second[8*model_obj.weight_precision_bytes])\n",
        "ttft_seconds = token_latency_seconds_default(np.array([n_gpu]), model_obj, gpu, np.array([batch_size]), \\\n",
        "                                              input_len=0, seq_len=input_len)\n",
        "output_tok_latency_seconds = spec_dec_token_latency_seconds(np.array([n_gpu]), model_obj, approx_model_obj, gpu, np.array([batch_size]), \\\n",
        "                                                            acceptance_prob=0.8, gamma_max=0, input_len=input_len)\n",
        "\n",
        "total_latency_seconds = ttft_seconds + output_len*output_tok_latency_seconds\n",
        "\n",
        "print(\"Number of GPUs:\", n_gpu)\n",
        "print(\"Batch size:\", batch_size)\n",
        "print(\"Tokens per second:\", output_len/total_latency_seconds)\n",
        "print(\"Requests per second:\", batch_size/total_latency_seconds)\n",
        "print(\"Throughput (overall tok/s):\", output_len*batch_size/total_latency_seconds)\n",
        "print(\"Throughput per GPU (overall tok/s):\", output_len*batch_size/(total_latency_seconds*n_gpu))\n",
        "print(\"Total latency:\", total_latency_seconds)\n",
        "print(\"Utilization rate:\", (model_obj.arithmetic_cost_flop(0, input_len*batch_size) + model_obj.arithmetic_cost_flop(input_len, output_len*batch_size))\n",
        "                            /(n_gpu*gpu.theoretical_flop_per_second[8*model_obj.weight_precision_bytes]*total_latency_seconds))\n",
        "print(\"Price per M output tokens (USD):\", 1e6 * gpu.price_dollars_per_hour / (3600*output_len*batch_size/(total_latency_seconds*n_gpu)))\n",
        "print(\"Output token latency (seconds):\", output_tok_latency_seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model testing cell\n",
        "\n",
        "Here, you can test the theoretical performance of some specific inference setup by playing with the values of all of the variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens per second: [8.78126821]\n",
            "Requests per second: [137.20731574]\n",
            "Throughput (overall tok/s): [17562.53641502]\n",
            "Throughput per GPU (overall tok/s): [390.278587]\n",
            "Total latency: [14.57648223]\n",
            "Utilization rate: [0.21451352]\n",
            "Price per M output tokens (USD): [1.49465882]\n",
            "Output token latency (seconds): [0.11379702]\n"
          ]
        }
      ],
      "source": [
        "input_len = 0\n",
        "output_len = 128\n",
        "\n",
        "batch_size = 2000\n",
        "\n",
        "model_obj = GPT_4\n",
        "approx_model_obj = Llama_3_8B\n",
        "gpu = H100_ZL\n",
        "n_gpu = 45\n",
        "\n",
        "#ttft_seconds = model_obj.arithmetic_cost_flop(0, input_len*batch_size)/(n_gpu*gpu.flop_per_second[8*model_obj.weight_precision_bytes])\n",
        "ttft_seconds = token_latency_seconds_default(np.array([n_gpu]), model_obj, gpu, np.array([batch_size]), \\\n",
        "                                              input_len=0, seq_len=input_len)\n",
        "output_tok_latency_seconds = spec_dec_token_latency_seconds(np.array([n_gpu]), model_obj, approx_model_obj, gpu, np.array([batch_size]), \\\n",
        "                                                            acceptance_prob=0.8, gamma_max=0, input_len=input_len)\n",
        "\n",
        "total_latency_seconds = ttft_seconds + output_len*output_tok_latency_seconds\n",
        "\n",
        "print(\"Tokens per second:\", output_len/total_latency_seconds)\n",
        "print(\"Requests per second:\", batch_size/total_latency_seconds)\n",
        "print(\"Throughput (overall tok/s):\", output_len*batch_size/total_latency_seconds)\n",
        "print(\"Throughput per GPU (overall tok/s):\", output_len*batch_size/(total_latency_seconds*n_gpu))\n",
        "print(\"Total latency:\", total_latency_seconds)\n",
        "print(\"Utilization rate:\", (model_obj.arithmetic_cost_flop(0, input_len*batch_size) + model_obj.arithmetic_cost_flop(input_len, output_len*batch_size))\n",
        "                            /(n_gpu*gpu.theoretical_flop_per_second[8*model_obj.weight_precision_bytes]*total_latency_seconds))\n",
        "print(\"Price per M output tokens (USD):\", 1e6 * gpu.price_dollars_per_hour / (3600*output_len*batch_size/(total_latency_seconds*n_gpu)))\n",
        "print(\"Output token latency (seconds):\", output_tok_latency_seconds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token economics plots\n",
        "\n",
        "The two cells below can be used to produce the plots and the information in the tables from the paper that compare the token economics of different models, different GPUs, *et cetera* with each other. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEbGfXHlApwQ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TokenEconSettings:\n",
        "  name: str\n",
        "  gpu: GPU\n",
        "  model: Model\n",
        "  approx_model: Model\n",
        "  input_len: int\n",
        "  max_throughput_tokens_per_second: float\n",
        "  observed_perf: list[tuple[float]] # list consisting of tuples of the form (tokens/sec/request, price in dollars/million output tokens)\n",
        "  color: str\n",
        "  spec_dec: bool\n",
        "  acceptance_prob: float\n",
        "\n",
        "  def __init__(self, name, gpu, model, input_len=0, max_throughput_tokens_per_second=np.infty, observed_perf=None, color=\"\", spec_dec=False, approx_model=None, acceptance_prob=1):\n",
        "    self.name = name\n",
        "    self.gpu = gpu\n",
        "    self.model = model\n",
        "    self.input_len = input_len\n",
        "    self.max_throughput_tokens_per_second = max_throughput_tokens_per_second\n",
        "    self.observed_perf = observed_perf\n",
        "    self.color = color\n",
        "    self.spec_dec = spec_dec\n",
        "    self.acceptance_prob = acceptance_prob\n",
        "\n",
        "    if approx_model != None:\n",
        "      self.approx_model = approx_model\n",
        "\n",
        "llama_comparison = [TokenEconSettings(name=\"Llama 3 70B (1M tok/sec)\", gpu=H100, model=Llama_3_70B, input_len=1, max_throughput_tokens_per_second=1e6, color=\"darkblue\"),\n",
        "                    TokenEconSettings(name=\"Llama 3 70B (10K tok/sec)\", gpu=H100, model=Llama_3_70B, input_len=1, max_throughput_tokens_per_second=1e4, color=\"blue\"),\n",
        "                    #TokenEconSettings(name=\"Llama 3 70B 8-bit (1M tok/sec)\", gpu=H100, model=Llama_3_70B_8_bit, input_len=1, max_throughput_tokens_per_second=1e6),\n",
        "                    #TokenEconSettings(name=\"Llama 3 70B 8-bit (10K tok/sec)\", gpu=H100, model=Llama_3_70B_8_bit, input_len=1, max_throughput_tokens_per_second=1e4),\n",
        "                    TokenEconSettings(name=\"Llama 3 8B (1M tok/sec)\", gpu=H100, model=Llama_3_8B, input_len=1, max_throughput_tokens_per_second=1e6, color=\"darkred\"),\n",
        "                    TokenEconSettings(name=\"Llama 3 8B (10K tok/sec)\", gpu=H100, model=Llama_3_8B, input_len=1, max_throughput_tokens_per_second=1e4, color=\"red\")]\n",
        "\n",
        "all_models_comparison = [TokenEconSettings(name=\"GPT-4 (speculative)\", gpu=H100, model=GPT_4, observed_perf=[(21, 37.5)], color=\"blue\"),\n",
        "                         TokenEconSettings(name=\"GPT-3\", gpu=H100, model=GPT_3, color=\"orange\"),\n",
        "                         TokenEconSettings(name=\"Llama 3.1 405B\", gpu=H100, model=Llama_3_405B, color=\"purple\"),\n",
        "                         TokenEconSettings(name=\"Llama 3 70B 8-bit\", gpu=H100, model=Llama_3_70B_8_bit, observed_perf=[(158, 0.9)], color=\"red\"),\n",
        "                         TokenEconSettings(name=\"Mixtral 8x22B\", gpu=H100, model=Mixtral_8x22B, observed_perf=[(89.25, 1.2)], color=\"green\")\n",
        "                         #TokenEconSettings(name=\"Llama 3 8B\", gpu=H100, model=Llama_3_8B, observed_perf=[(374, 0.2)], color=\"black\")\n",
        "                         ]\n",
        "\n",
        "gpus_comparison = [TokenEconSettings(name=\"H100\", gpu=H100, model=Llama_3_405B_8_bit, input_len=0, max_throughput_tokens_per_second=np.infty, color=\"blue\"),\n",
        "                   TokenEconSettings(name=\"A100\", gpu=A100, model=Llama_3_405B_8_bit, input_len=0, max_throughput_tokens_per_second=np.infty, color=\"green\"),\n",
        "                   TokenEconSettings(name=\"V100\", gpu=V100, model=Llama_3_405B_8_bit, input_len=0, max_throughput_tokens_per_second=np.infty, color=\"red\"),\n",
        "                   TokenEconSettings(name=\"Groq LPU\", gpu=Groq_LPU, model=Llama_3_405B_8_bit, input_len=0, max_throughput_tokens_per_second=np.infty, color=\"purple\")]\n",
        "\n",
        "context_length_comparison = [TokenEconSettings(name=\"Empty context\", gpu=H100, model=Llama_3_70B_8_bit, input_len=0, max_throughput_tokens_per_second=np.infty, color=\"blue\"),\n",
        "                             TokenEconSettings(name=\"Context of 1K tokens\", gpu=H100, model=Llama_3_70B_8_bit, input_len=1000, max_throughput_tokens_per_second=np.infty, color=\"green\"),\n",
        "                             TokenEconSettings(name=\"Context of 10K tokens\", gpu=H100, model=Llama_3_70B_8_bit, input_len=10000, max_throughput_tokens_per_second=np.infty, color=\"red\")]\n",
        "\n",
        "quantization_comparison = [TokenEconSettings(name=\"Llama 3 70B 16-bit\", gpu=H100, model=Llama_3_70B, input_len=0, color=\"red\"),\n",
        "                           TokenEconSettings(name=\"Llama 3 70B 8-bit\", gpu=H100, model=Llama_3_70B_8_bit, input_len=0, color=\"green\"),\n",
        "                           TokenEconSettings(name=\"Llama 3 70B 4-bit\", gpu=H100, model=Llama_3_70B_4_bit, input_len=0, color=\"blue\")]\n",
        "\n",
        "spec_dec_comparison = [TokenEconSettings(name=\"Llama 3 70B (8B acceptance=0.8)\", gpu=H100, model=Llama_3_70B, approx_model=Llama_3_8B, spec_dec=True, acceptance_prob=0.8, color=\"red\"),\n",
        "                       TokenEconSettings(name=\"Llama 3 70B (no spec dec)\", gpu=H100, model=Llama_3_70B, approx_model=Llama_3_8B, spec_dec=True, acceptance_prob=0, color=\"gray\"),\n",
        "                       TokenEconSettings(name=\"Llama 3.1 405B (8B acceptance=0.8)\", gpu=H100, model=Llama_3_405B, approx_model=Llama_3_8B, spec_dec=True, acceptance_prob=0.8, color=\"blue\"),\n",
        "                       TokenEconSettings(name=\"Llama 3.1 405B (no spec dec)\", gpu=H100, model=Llama_3_405B, approx_model=Llama_3_70B, spec_dec=True, acceptance_prob=0, color=\"black\")]\n",
        "\n",
        "\n",
        "tpu_test = [TokenEconSettings(name=\"Llama 3.1 405B\", gpu=TPU_v4, model=Llama_3_405B, approx_model=Llama_3_70B, spec_dec=True, acceptance_prob=0, color=\"black\")]\n",
        "\n",
        "def pareto_fronts(comparison_list: list[TokenEconSettings], token_latency_seconds_func):\n",
        "  token_economics_results = []\n",
        "\n",
        "  for comparison_setting in comparison_list:\n",
        "    gpu = comparison_setting.gpu\n",
        "    model = comparison_setting.model\n",
        "    input_len = comparison_setting.input_len\n",
        "    max_throughput_tokens_per_second = comparison_setting.max_throughput_tokens_per_second\n",
        "\n",
        "    # min_num_of_gpus = np.maximum(1, model.total_params*model.precision_bytes/gpu.hbm_size_bytes)\n",
        "    min_num_of_gpus = model.total_params*model.weight_precision_bytes/gpu.hbm_size_bytes\n",
        "\n",
        "    batch_size_range = np.logspace(0, 18 + np.log2(model.sparsity_factor), num=400, base=2)\n",
        "    n_gpu_range = np.logspace(np.log2(min_num_of_gpus), 18, num=400, base=2)\n",
        "\n",
        "    batch_size_array = np.transpose(np.tile(batch_size_range, (len(n_gpu_range), 1)))\n",
        "    n_gpu_array = np.tile(n_gpu_range, (len(batch_size_range), 1))\n",
        "\n",
        "    assert np.shape(batch_size_array) == np.shape(n_gpu_array)\n",
        "\n",
        "    if comparison_setting.spec_dec:\n",
        "      token_latency_seconds_array = spec_dec_token_latency_seconds(n_gpu_array, model=model, approx_model=comparison_setting.approx_model, gpu=gpu, batch_size=batch_size_array, \\\n",
        "                                                               gamma_max=20, acceptance_prob=comparison_setting.acceptance_prob, input_len=input_len)\n",
        "    else:\n",
        "      token_latency_seconds_array = token_latency_seconds_func(n_gpu_array, model=model, gpu=gpu, batch_size=batch_size_array, input_len=input_len)\n",
        "\n",
        "    gpu_seconds_per_token = n_gpu_array * token_latency_seconds_array/batch_size_array\n",
        "\n",
        "    pareto_front_gpu_seconds_per_token = []\n",
        "    token_latency_valid = []\n",
        "    gpu_counts = []\n",
        "    batch_sizes = []\n",
        "\n",
        "    token_latency_seconds_range = 1e-3 * np.logspace(-1, 3, base=10, num=600)\n",
        "\n",
        "    for token_latency_seconds_sample in token_latency_seconds_range:\n",
        "      indices = np.where((token_latency_seconds_array <= token_latency_seconds_sample) & (batch_size_array/token_latency_seconds_array <= max_throughput_tokens_per_second))\n",
        "      if len(indices[0]) > 0:\n",
        "        minimal_cost = np.min(gpu_seconds_per_token[indices])\n",
        "        token_latency_found = np.min(token_latency_seconds_array[np.where(gpu_seconds_per_token == minimal_cost)])\n",
        "        found_index = np.argmin(token_latency_seconds_array[np.where(gpu_seconds_per_token == minimal_cost)])\n",
        "\n",
        "        if token_latency_found not in token_latency_valid:\n",
        "          pareto_front_gpu_seconds_per_token.append(minimal_cost)\n",
        "          token_latency_valid.append(token_latency_found)\n",
        "          gpu_counts.append(n_gpu_array[np.where(gpu_seconds_per_token == minimal_cost)][found_index])\n",
        "          batch_sizes.append(batch_size_array[np.where(gpu_seconds_per_token == minimal_cost)][found_index])\n",
        "\n",
        "    token_latency_valid = np.array(token_latency_valid)\n",
        "    pareto_front_gpu_seconds_per_token = np.array(pareto_front_gpu_seconds_per_token)\n",
        "\n",
        "    gpu_price_dollars_per_second = gpu.price_dollars_per_hour/3600\n",
        "\n",
        "    x_coords = 1/token_latency_valid\n",
        "    y_coords = 1e6*pareto_front_gpu_seconds_per_token*gpu_price_dollars_per_second\n",
        "    mfu_values = model.arithmetic_cost_flop(input_len, np.array(batch_sizes))/(np.array(gpu_counts)*gpu.theoretical_flop_per_second[model.weight_precision_bytes*8]*token_latency_valid)\n",
        "    token_economics_results.append((x_coords, y_coords, gpu_counts, batch_sizes, mfu_values))\n",
        "\n",
        "  return token_economics_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "ad-RBCQhJD9S",
        "outputId": "97cb9abd-7f5f-4693-f46b-b08f44ba642a"
      },
      "outputs": [],
      "source": [
        "def user_preference_intensity(tokens_per_second_per_request: float, price_per_million_tokens: float):\n",
        "   return tokens_per_second_per_request**4 / price_per_million_tokens\n",
        "\n",
        "def preference_maximizing_settings(comparison, token_economics_results):\n",
        "  results = dict()\n",
        "  for (comparison_setting, (x_coords, y_coords, gpu_counts, batch_sizes, mfu_values)) in zip(comparison, token_economics_results):\n",
        "    maximizing_index = np.argmax(user_preference_intensity(x_coords, y_coords))\n",
        "    results[comparison_setting.name] = {\"tokens_per_second_per_request\": x_coords[maximizing_index], \\\n",
        "                                        \"price_dollars_per_million_tokens\": y_coords[maximizing_index], \\\n",
        "                                        \"gpus_per_instance\": gpu_counts[maximizing_index], \\\n",
        "                                        \"batch_size\": batch_sizes[maximizing_index], \\\n",
        "                                        \"utilization_rate\": mfu_values[maximizing_index]\n",
        "                                       }\n",
        "  return results\n",
        "\n",
        "#token_economics_results = pareto_fronts(llama_comparison, section_2_2_token_latency_seconds)\n",
        "#current_comparison = gpus_comparison\n",
        "#current_comparison = all_models_comparison\n",
        "#current_comparison = context_length_comparison\n",
        "#current_comparison = llama_comparison\n",
        "#current_comparison = quantization_comparison\n",
        "current_comparison = spec_dec_comparison\n",
        "\n",
        "#current_comparison = tpu_test\n",
        "\n",
        "token_economics_results = pareto_fronts(current_comparison, token_latency_seconds_default)\n",
        "#token_economics_results = pareto_fronts(current_comparison, section_2_2_token_latency_seconds)\n",
        "\n",
        "min_y = min([min(y_coords) for (x_coords, y_coords, gpu_counts, batch_sizes, mfu_values) in token_economics_results])\n",
        "max_y = max([max(y_coords) for (x_coords, y_coords, gpu_counts, batch_sizes, mfu_values) in token_economics_results])\n",
        "\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "\n",
        "for (comparison_setting, (x_coords, y_coords, gpu_counts, batch_sizes, mfu_values)) in zip(current_comparison, token_economics_results):\n",
        "  plt.plot(x_coords, y_coords, label=comparison_setting.name, color=comparison_setting.color)\n",
        "  if comparison_setting.observed_perf != None:\n",
        "    x_l = []\n",
        "    y_l = []\n",
        "    for (x, y) in comparison_setting.observed_perf:\n",
        "      x_l.append(x)\n",
        "      y_l.append(y)\n",
        "\n",
        "    plt.scatter(x_l, y_l, color=comparison_setting.color)\n",
        "\n",
        "  opt_index = np.argmax(x_coords)\n",
        "  print(\"Maximum throughput for %s: %.2f tokens/second/request (using %.2f GPUs at a batch size of %.2f)\" % (comparison_setting.name, x_coords[opt_index], \\\n",
        "                                                                                                             gpu_counts[opt_index], batch_sizes[opt_index]))\n",
        "print(\"\\n\")\n",
        "\n",
        "preference_max_results = preference_maximizing_settings(current_comparison, token_economics_results)\n",
        "for setting_name in preference_max_results:\n",
        "  results_dict = preference_max_results[setting_name]\n",
        "  print(\"Preferred throughput for %s: %.2f tokens/second/request at %.2f USD/million output tokens (using %.2f GPUs at a batch size of %.2f)\" % (setting_name, results_dict[\"tokens_per_second_per_request\"], \\\n",
        "                                                                                                                                                 results_dict[\"price_dollars_per_million_tokens\"], \\\n",
        "                                                                                                                                                 results_dict[\"gpus_per_instance\"], \\\n",
        "                                                                                                                                                 results_dict[\"batch_size\"]))\n",
        "\n",
        "plt.xlabel(\"Tokens per second per request\")\n",
        "plt.ylabel(\"Marginal cost per million tokens generated (dollars)\")\n",
        "\n",
        "plt.yscale(\"log\")\n",
        "plt.gca().set_yticks([tick for tick in np.logspace(start=-10, stop=10, num=21, base=2) if tick <= max_y and tick >= min_y])\n",
        "plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda y, _: '%.3f' % y))\n",
        "\n",
        "plt.title(\"Token economics: Llama 3.1 405B at 8-bit precision on different hardware (no spec dec)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"token_economics_gpu_comparison.pdf\")\n",
        "plt.savefig(\"token_economics_gpu_comparison.png\")\n",
        "plt.show()\n",
        "\n",
        "for (comparison_setting, (x_coords, y_coords, gpu_counts, batch_sizes, mfu_values)) in zip(current_comparison, token_economics_results):\n",
        "  plt.plot(x_coords, mfu_values, label=comparison_setting.name, color=comparison_setting.color)\n",
        "\n",
        "plt.xlabel(\"Tokens per second per request\")\n",
        "plt.ylabel(\"MFU rate\")\n",
        "\n",
        "plt.title(\"Token economics: Llama 3.1 405B at 8-bit precision on different hardware (no spec dec)\")\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"token_economics_gpu_comparison_mfu.pdf\")\n",
        "plt.savefig(\"token_economics_gpu_comparison_mfu.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
